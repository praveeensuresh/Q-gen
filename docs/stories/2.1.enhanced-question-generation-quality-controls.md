# Story 2.1: Enhanced Question Generation with Quality Controls

## Status
Draft

## Story
**As a** teacher,
**I want** the AI to generate high-quality, varied question types with better prompts,
**so that** the questions are educationally sound and ready for classroom use.

## Acceptance Criteria
1. Enhanced prompt templates generate diverse question types (MCQ, short answer, true/false)
2. Question difficulty levels are appropriate for the content complexity
3. Generated questions include clear, unambiguous answer choices for MCQs
4. Questions are properly formatted with consistent numbering and structure
5. AI responses are validated for educational appropriateness
6. Question generation handles different content types (text, lists, definitions)

## Tasks / Subtasks
- [ ] Develop enhanced prompt templates (AC: 1)
  - [ ] Create specialized prompts for each question type
  - [ ] Add content-specific prompt variations
  - [ ] Implement dynamic prompt selection based on content
- [ ] Implement question difficulty assessment (AC: 2)
  - [ ] Create difficulty assessment algorithm
  - [ ] Add content complexity analysis
  - [ ] Implement difficulty-based prompt selection
- [ ] Add answer choice validation for MCQs (AC: 3)
  - [ ] Validate answer choice clarity and distinctiveness
  - [ ] Ensure correct answer is not obvious
  - [ ] Add distractor quality assessment
- [ ] Create question formatting and numbering system (AC: 4)
  - [ ] Implement consistent question numbering
  - [ ] Add proper formatting for different question types
  - [ ] Create structured question layout
- [ ] Implement educational content validation (AC: 5)
  - [ ] Add appropriateness checking
  - [ ] Validate educational relevance
  - [ ] Filter inappropriate content
- [ ] Add support for different content types (AC: 6)
  - [ ] Handle text-based content
  - [ ] Process list and definition content
  - [ ] Add content type-specific prompts
- [ ] Create question quality scoring system (AC: 5)
  - [ ] Implement quality metrics
  - [ ] Add scoring algorithm
  - [ ] Create quality feedback system

## Dev Notes

### Previous Story Insights
[Source: Story 1.4 completion context]
- Basic question generation is complete with OpenAI integration
- Question display and export functionality is working
- Basic template system is in place
- QuestionSet and Question models are established

### Data Models
[Source: architecture.md#data-models]

**Enhanced Question Model:**
```typescript
interface Question {
  id: string;
  question_set_id: string;
  question_type: 'multiple_choice' | 'short_answer' | 'true_false';
  question_text: string;
  correct_answer: string;
  options: string[]; // For multiple choice
  difficulty: 'easy' | 'medium' | 'hard';
  order_index: number;
  created_at: Date;
  quality_score?: number; // New field for quality assessment
  content_type?: 'text' | 'list' | 'definition' | 'mixed'; // New field
}
```

**Quality Assessment Model:**
```typescript
interface QualityAssessment {
  question_id: string;
  clarity_score: number;
  difficulty_appropriateness: number;
  educational_value: number;
  overall_score: number;
  feedback: string[];
}
```

### API Specifications
[Source: architecture.md#api-specification]

**Enhanced Endpoints:**
- `POST /api/question-sets` - Enhanced question generation with quality controls
- `POST /api/question-sets/{id}/regenerate` - Regenerate questions with improved quality
- `GET /api/question-sets/{id}/quality-report` - Get quality assessment report

**Enhanced Request Format:**
```typescript
interface EnhancedGenerationRequest {
  document_id: string;
  question_count: number;
  question_types: ('multiple_choice' | 'short_answer' | 'true_false')[];
  difficulty_levels: ('easy' | 'medium' | 'hard')[];
  content_types: ('text' | 'list' | 'definition' | 'mixed')[];
  quality_threshold: number;
  custom_prompts?: {
    [question_type: string]: string;
  };
}
```

### Component Specifications
[Source: architecture.md#components]

**Enhanced Question Generation Component:**
- Advanced configuration options for question generation
- Quality control settings and preview
- Technology Stack: MUI Form components, advanced controls

**Quality Assessment Component:**
- Display quality scores and feedback
- Allow manual quality adjustments
- Technology Stack: MUI DataGrid, scoring displays

### File Locations
[Source: architecture.md#unified-project-structure]

**Enhanced Service Files:**
```
apps/web/src/services/
├── enhancedAiService.ts
├── qualityAssessmentService.ts
├── promptTemplateService.ts
└── difficultyAssessmentService.ts
```

**Template Files:**
```
apps/web/src/templates/prompts/
├── multiple-choice-enhanced.txt
├── short-answer-enhanced.txt
├── true-false-enhanced.txt
└── content-specific/
    ├── text-based.txt
    ├── list-based.txt
    └── definition-based.txt
```

### Testing Requirements
[Source: architecture.md#testing-strategy]

**Testing Standards:**
- Test file location: `apps/web/src/__tests__/` for unit tests
- Testing frameworks: Vitest + Testing Library for unit and integration tests
- Unit test coverage: Minimum 85% code coverage for new functionality (higher than Epic 1)

**Testing Requirements for this story:**
- Unit tests for enhanced prompt templates
- Unit tests for quality assessment algorithms
- Unit tests for difficulty assessment logic
- Integration tests for enhanced question generation
- Test quality scoring with various question types

### Technical Constraints
[Source: architecture.md#tech-stack]

**Enhanced AI Configuration:**
- Model: GPT-4 with enhanced prompting
- Temperature: 0.5 for more consistent quality
- Max tokens: 3000 per request for detailed responses
- Enhanced prompt engineering for better results

**Environment Variables:**
```bash
OPENAI_API_KEY=your_openai_api_key
QUALITY_THRESHOLD=0.7
MAX_QUESTION_VARIETY=5
ENHANCED_PROMPTS_ENABLED=true
```

**Performance Requirements:**
- Enhanced generation should complete within 45 seconds
- Quality assessment should be real-time
- Support for up to 50 questions with quality controls

### Error Scenarios
[Source: prd-epic2-stories.md#story-21]

**Error Handling Requirements:**
- Insufficient Question Variety: "Unable to generate requested question types. Please try with different content or settings"
- Difficulty Assessment Failure: "Difficulty level could not be determined. Questions will use default difficulty"
- Ambiguous Answer Choices: "Some answer choices are unclear. Please review and edit questions"
- Formatting Inconsistencies: "Question formatting issues detected. Please review the generated questions"
- Inappropriate Content Detection: "Some content may not be suitable for educational use. Please review and regenerate"
- Content Type Recognition Failure: "Unable to process this content type effectively. Please try with text-based content"
- Question Quality Below Threshold: "Generated questions may not meet quality standards. Please review or try again"
- Prompt Template Error: "Question generation template error. Please try again or contact support"
- Partial Generation Failure: "Some questions could not be generated. Showing available questions"

## Testing

### Testing Standards
[Source: architecture.md#testing-strategy]

**Test File Location:**
- Unit tests: `apps/web/src/__tests__/`
- Integration tests: `apps/web/src/__tests__/integration/`
- E2E tests: `apps/web/e2e/`

**Testing Frameworks:**
- Unit/Integration: Vitest + Testing Library
- E2E: Playwright (for future stories)

**Test Coverage Requirements:**
- Minimum 85% code coverage for new functionality
- All quality assessment functions must have unit tests
- All error scenarios must be tested

**Testing Patterns:**
- Test enhanced prompts with various content types
- Validate quality scoring algorithms
- Test difficulty assessment with different content
- Mock AI responses for consistent testing

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| [Current Date] | 1.0 | Initial story creation | Scrum Master |

## Dev Agent Record

### Agent Model Used
_To be populated by development agent_

### Debug Log References
_To be populated by development agent_

### Completion Notes List
_To be populated by development agent_

### File List
_To be populated by development agent_

## QA Results
_To be populated by QA agent_
